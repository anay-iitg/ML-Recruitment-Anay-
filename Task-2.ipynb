{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d32139ac",
   "metadata": {},
   "source": [
    "# TASK 2 #\n",
    "\n",
    "I recommend restarting the kernel before running each level \n",
    "\n",
    "## Level 1: Core Activation ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d41bd",
   "metadata": {},
   "source": [
    "## What all I read before this task ##\n",
    "I read the documentation, downloaded the API Keys and tried to understand what each block of code does(I don't exactly know how they work, but I have a pretty good idea what each block of code does and use for it)\n",
    "\n",
    "I have also hard coded all the API keys (did **a lot** of requests so the free subscription might be on the verge of finishing, so use carefully XD )\n",
    "\n",
    "And one thing I didn't like was that why tf are they changing libraries and locations of classes so fast, installed the latest versions but in 2-3 days, had to update some again! And the location of the classes is also not fixed!!\n",
    "\n",
    "## Challenges I faced for adding the tool ##\n",
    "- I was trying to do like the Tavily search tool coded in the langgraph documentation. Was struggling a lot as I didn't know that the commented section under triple quotes is the description, I was removing that again and again from the documentation, finally got it right tho lol. I also read this - https://python.langchain.com/docs/concepts/tools/#tool-interface and referred material under Pre-built agents\n",
    "\n",
    "**I have read almost all the things from documentation only, so everyhting is similar to that**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4183d060",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.tracers.stdout import ConsoleCallbackHandler\n",
    "from langchain_core.messages import AIMessage, ToolMessage, HumanMessage\n",
    "\n",
    "@tool\n",
    "def calculator_tool(expression: str):\n",
    "    \"\"\"\n",
    "    Evaluates a mathematical expression and returns the result.\n",
    "    The input should be a string that can be safely evaluated by Python's eval(),\n",
    "    e.g., '2 + 2', '10 / 5 * 2'.\n",
    "    \"\"\"\n",
    "    \n",
    "    result = eval(expression)\n",
    "    return float(result)\n",
    "    \n",
    "\n",
    "tools = [calculator_tool]\n",
    "#calculator_tool.invoke(\"4+(3*2)/4\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a448bdb",
   "metadata": {},
   "source": [
    "I have copy pasted the below code cell from the documentation. Have a high level idea what each part is doing and hope to learn properly in the coding club..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30948921",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDEF3eBIfkVtIZj5hR-RDHdN2nkMj2Emxo\"\n",
    "\n",
    "#handler = ConsoleCallbackHandler()\n",
    "\n",
    "#llm = init_chat_model(\"google_genai:gemini-2.0-flash\",callbacks=[handler])\n",
    "llm = init_chat_model(\"google_genai:gemini-2.0-flash\")\n",
    "\n",
    "\n",
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tool_node =ToolNode(tools)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "def route_tools(\n",
    "    state: State,\n",
    "):\n",
    "    \"\"\"\n",
    "    Use in the conditional_edge to route to the ToolNode if the last message\n",
    "    has tool calls. Otherwise, route to the end.\n",
    "    \"\"\"\n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "    elif messages := state.get(\"messages\", []):\n",
    "        ai_message = messages[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    route_tools,\n",
    "    \n",
    "    {\"tools\": \"tools\", END: END},\n",
    ")\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph = graph_builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "178a4a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38ed4d6",
   "metadata": {},
   "source": [
    "## What does this code of block do? ##\n",
    "Again, I have taken the code from the documentation, so what the code does is it asks user for the input, sends it to the LLM by graph (in a structured way with role, user, content etc.) which sends back it's response, this keeps on going. If the user inputs 'quit', 'exit' or these type of codes the loop breaks, the loop also breaks if there is some error(when i directly press enter without typing in anything), in this case the user input is hardcoded.\n",
    "\n",
    "# Issues that I faced here:\n",
    "Although this works and the response is correct but it doesn't show a proper structured output, for ex for the input 'What is 1+2*3?', the response is:\n",
    " - User: What is 1+2*3?\n",
    " - Assistant:\n",
    "- Assistant: 7.0\n",
    " - Assistant: 1+2*3 is 7.\n",
    "\n",
    " Though it is clearly visible that it has accessed a tool as one message is for calling the tool, sending tool the information, one is the tool response and one is the response by AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0002f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def stream_graph_updates(user_input: str):\\n    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\\n        for value in event.values():\\n            print(\"Assistant:\", value[\"messages\"][-1].content)\\n\\nwhile True:\\n    try:\\n        user_input = input(\"User: \")\\n        if user_input.lower() in [\"quit\", \"exit\", \"q\", \"chal bye\",\"alvida\",\"chal theek hai\"]:\\n            print(\"Goodbye!\")\\n            break\\n\\n        stream_graph_updates(user_input)\\n    except:\\n        # fallback if input() is not available\\n        user_input = \"What is 1+2*3?\"\\n        print(\"User: \" + user_input)\\n        stream_graph_updates(user_input)\\n        break'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is the code which gives an unstructured output\n",
    "\"\"\"def stream_graph_updates(user_input: str):\n",
    "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\", \"chal bye\",\"alvida\",\"chal theek hai\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        stream_graph_updates(user_input)\n",
    "    except:\n",
    "        # fallback if input() is not available\n",
    "        user_input = \"What is 1+2*3?\"\n",
    "        print(\"User: \" + user_input)\n",
    "        stream_graph_updates(user_input)\n",
    "        break\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf7312b",
   "metadata": {},
   "source": [
    "## THE SOLUTION: ##\n",
    "\n",
    "I asked an AI this, **I didn't provide my code**, now did I ask to update/check my code, I only wanted the required information and functions. I have also attached the prompt below:\n",
    "\n",
    "Prompt:\n",
    "I have made a interactive chatbot using Langgraph, I have also integrated certain tools with it, how do I know when the chatbot is accessing these tools, I want a proper structured output \n",
    "\n",
    "In response I got two methods, one was through consolecallbackhandler, it worked **I have even commented out the 3 lines of code** but it was very messy, so I used the other method which categorizes the last message into AI message and Tool message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e37f291f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: what is 1+1?\n",
      "Assistant: Giving info to tool\n",
      "Assistant: Calculator Tool gave 2.0\n",
      "Assistant: 2\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "#this code will give structured output:\n",
    "def stream_graph_updates(user_input: str):\n",
    "    print(f\"User: {user_input}\")\n",
    "\n",
    "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
    "        for value in event.values():\n",
    "            if \"messages\" in value:\n",
    "                last_message = value[\"messages\"][-1]\n",
    "\n",
    "                # If it is calling a tool\n",
    "                if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "                    print(\"Assistant: Giving info to tool\")\n",
    "                    \n",
    "                # The tool has retured it's result\n",
    "                elif isinstance(last_message, ToolMessage):\n",
    "                    tool_output = last_message.content\n",
    "                    print(f\"Assistant: Calculator Tool gave {tool_output}\")\n",
    "\n",
    "                # Answer by chatbot\n",
    "                elif isinstance(last_message, AIMessage) :\n",
    "                    print(f\"Assistant: {last_message.content}\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"\") \n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\", \"chal bye\",\"alvida\",\"chal theek hai\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        stream_graph_updates(user_input)\n",
    "    except Exception as e:\n",
    "        user_input = \"What is 1+2*3?\"\n",
    "        stream_graph_updates(user_input)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13d5253",
   "metadata": {},
   "source": [
    "## Level 2: Senses of the World ##\n",
    "\n",
    "So for this I used tavily, integrating it was simple as everything was mentioned in the document and for weather, I didn't initially see that it was given in the PS, so I used a youtube video for getting the API key for openweather and how to use it, then I put it in the form of a tool. I am attaching the link - https://www.youtube.com/watch?v=9P5MY_2i7K8 This cannot give the forecast or past weather tho :(\n",
    "\n",
    "I have also commented out the part to test the code by invoking the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04f3b7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.messages import AIMessage, ToolMessage, HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "import requests\n",
    "\n",
    "@tool\n",
    "def fashion_tool(region: str):\n",
    "    \"\"\"\n",
    "    This function uses the TavilySearch tool to fetch fashion and dressing related content.\n",
    "    The input should be a string representing the region or topic of interest.\n",
    "    Output contains a JSON string of results which has a breif description of the content as Answer.\n",
    "    \"\"\"\n",
    "    from langchain_tavily import TavilySearch\n",
    "    #os.environ['TAVILY_API_KEY']\n",
    "    tool = TavilySearch(\n",
    "        tavily_api_key = \"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\",\n",
    "        max_results=5,\n",
    "        topic=\"general\",\n",
    "        include_answer=True,\n",
    "    )\n",
    "    model_generated_tool_call = {\n",
    "        \"args\": {\"query\": f\"What kind of fashion is trending in {region}\"},\n",
    "        \"id\": \"1\",\n",
    "        \"name\": \"tavily\",\n",
    "        \"type\": \"tool_call\",\n",
    "    }\n",
    "    tool_msg = tool.invoke(model_generated_tool_call)\n",
    "    return tool_msg.content\n",
    "#print(fashion_tool.invoke(\"Pune\"))\n",
    "    \n",
    "\n",
    "\n",
    "@tool\n",
    "def weather_tool(city: str):\n",
    "    \"\"\"\n",
    "    Fetches the current weather for a given city using OpenWeather API.\n",
    "    The input should be a string representing the city name.\n",
    "    The function returns a dictionary with weather information.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        api_key = \"2e7f2624eab23a98351aea15d372983f\"\n",
    "        BASE_URL = \"https://api.openweathermap.org/data/2.5/weather?\"\n",
    "        url = BASE_URL + \"appid=\" + api_key + \"&q=\" + city\n",
    "        response = requests.get(url).json()\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching weather data: {e}\"\n",
    "\n",
    "#print(weather_tool.invoke(\"London\"))\n",
    "\n",
    "@tool\n",
    "def calculator_tool(expression: str):\n",
    "    \"\"\"\n",
    "    Evaluates a mathematical expression and returns the result.\n",
    "    The input should be a string that can be safely evaluated by Python's eval(),\n",
    "    e.g., '2 + 2', '10 / 5 * 2'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return float(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error evaluating expression: {e}\"\n",
    "\n",
    "tools = [calculator_tool, weather_tool, fashion_tool]\n",
    "#calculator_tool.invoke(\"4+(3*2)/4\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8542e4d1",
   "metadata": {},
   "source": [
    "The two code blocks below are the same, no need in changing that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0c0d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDEF3eBIfkVtIZj5hR-RDHdN2nkMj2Emxo\"\n",
    "\n",
    "llm = init_chat_model(\"google_genai:gemini-2.0-flash\")\n",
    "\n",
    "\n",
    "\n",
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tool_node =ToolNode(tools)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "def route_tools(\n",
    "    state: State,\n",
    "):\n",
    "    \"\"\"\n",
    "    Use in the conditional_edge to route to the ToolNode if the last message\n",
    "    has tool calls. Otherwise, route to the end.\n",
    "    \"\"\"\n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "    elif messages := state.get(\"messages\", []):\n",
    "        ai_message = messages[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    route_tools,\n",
    "    {\"tools\": \"tools\", END: END},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph = graph_builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54a1d310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: hey there\n",
      "Assistant: Hi there! How can I help you today?\n",
      "User: what's the weather in pune?\n",
      "Assistant: Giving info to tool\n",
      "Assistant:Tool response: {\"coord\": {\"lon\": 73.8553, \"lat\": 18.5196}, \"weather\": [{\"id\": 804, \"main\": \"Clouds\", \"description\": \"overcast clouds\", \"icon\": \"04n\"}], \"base\": \"stations\", \"main\": {\"temp\": 295.42, \"feels_like\": 296.08, \"temp_min\": 295.42, \"temp_max\": 295.42, \"pressure\": 1006, \"humidity\": 91, \"sea_level\": 1006, \"grnd_level\": 933}, \"visibility\": 10000, \"wind\": {\"speed\": 3.38, \"deg\": 277, \"gust\": 8.09}, \"clouds\": {\"all\": 100}, \"dt\": 1748199343, \"sys\": {\"type\": 2, \"id\": 2096426, \"country\": \"IN\", \"sunrise\": 1748219295, \"sunset\": 1748266518}, \"timezone\": 19800, \"id\": 1259229, \"name\": \"Pune\", \"cod\": 200}\n",
      "Assistant: The weather in Pune is overcast with a temperature of 295.42K, which feels like 296.08K. The humidity is 91% and the wind is blowing from 277 degrees at a speed of 3.38 m/s.\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "#this code will give structured output:\n",
    "def stream_graph_updates(user_input: str):\n",
    "    print(f\"User: {user_input}\")\n",
    "\n",
    "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
    "        for value in event.values():\n",
    "            if \"messages\" in value:\n",
    "                last_message = value[\"messages\"][-1]\n",
    "\n",
    "                # If it is calling a tool\n",
    "                if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "                    print(\"Assistant: Giving info to tool\")\n",
    "                    \n",
    "                # The tool has retured it's result\n",
    "                elif isinstance(last_message, ToolMessage):\n",
    "                    tool_output = last_message.content\n",
    "                    print(f\"Assistant:Tool response: {tool_output}\")\n",
    "\n",
    "                # Answer by chatbot\n",
    "                elif isinstance(last_message, AIMessage) :\n",
    "                    print(f\"Assistant: {last_message.content}\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"\") \n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\", \"chal bye\",\"alvida\",\"chal theek hai\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        stream_graph_updates(user_input)\n",
    "    except Exception as e:\n",
    "        user_input = \"What is 1+2*3?\"\n",
    "        stream_graph_updates(user_input)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b136ac3e",
   "metadata": {},
   "source": [
    "## Level 3: Judgement and Memory ##\n",
    "\n",
    "The code is mostly similar, just have added a few lines extra to save the memory and update it in the graph, I have put comments before those lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d073efe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.messages import AIMessage, ToolMessage, HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "import requests\n",
    "\n",
    "@tool\n",
    "def fashion_tool(region: str):\n",
    "    \"\"\"\n",
    "    This function uses the TavilySearch tool to fetch fashion and dressing related content.\n",
    "    The input should be a string representing the region or topic of interest.\n",
    "    Output contains a JSON string of results which has a breif description of the content as Answer.\n",
    "    \"\"\"\n",
    "    from langchain_tavily import TavilySearch\n",
    "    #os.environ['TAVILY_API_KEY']\n",
    "    tool = TavilySearch(\n",
    "        tavily_api_key = \"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\",\n",
    "        max_results=5,\n",
    "        topic=\"general\",\n",
    "        include_answer=True,\n",
    "    )\n",
    "    model_generated_tool_call = {\n",
    "        \"args\": {\"query\": f\"What kind of fashion is trending in {region}\"},\n",
    "        \"id\": \"1\",\n",
    "        \"name\": \"tavily\",\n",
    "        \"type\": \"tool_call\",\n",
    "    }\n",
    "    tool_msg = tool.invoke(model_generated_tool_call)\n",
    "    return tool_msg.content\n",
    "#print(fashion_tool.invoke(\"Pune\"))\n",
    "    \n",
    "\n",
    "\n",
    "@tool\n",
    "def weather_tool(city: str):\n",
    "    \"\"\"\n",
    "    Fetches the current weather for a given city using OpenWeather API.\n",
    "    The input should be a string representing the city name.\n",
    "    The function returns a dictionary with weather information.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        api_key = \"2e7f2624eab23a98351aea15d372983f\"\n",
    "        BASE_URL = \"https://api.openweathermap.org/data/2.5/weather?\"\n",
    "        url = BASE_URL + \"appid=\" + api_key + \"&q=\" + city\n",
    "        response = requests.get(url).json()\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching weather data: {e}\"\n",
    "\n",
    "#print(weather_tool.invoke(\"London\"))\n",
    "\n",
    "@tool\n",
    "def calculator_tool(expression: str):\n",
    "    \"\"\"\n",
    "    Evaluates a mathematical expression and returns the result.\n",
    "    The input should be a string that can be safely evaluated by Python's eval(),\n",
    "    e.g., '2 + 2', '10 / 5 * 2'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return float(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error evaluating expression: {e}\"\n",
    "\n",
    "tools = [calculator_tool, weather_tool, fashion_tool]\n",
    "#calculator_tool.invoke(\"4+(3*2)/4\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a83c3d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDEF3eBIfkVtIZj5hR-RDHdN2nkMj2Emxo\"\n",
    "\n",
    "llm = init_chat_model(\"google_genai:gemini-2.0-flash\")\n",
    "\n",
    "\n",
    "\n",
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tool_node =ToolNode(tools)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "def route_tools(\n",
    "    state: State,\n",
    "):\n",
    "    \"\"\"\n",
    "    Use in the conditional_edge to route to the ToolNode if the last message\n",
    "    has tool calls. Otherwise, route to the end.\n",
    "    \"\"\"\n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "    elif messages := state.get(\"messages\", []):\n",
    "        ai_message = messages[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    route_tools,\n",
    "    {\"tools\": \"tools\", END: END},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "# the code below will save the graph in memory\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d91b65",
   "metadata": {},
   "source": [
    "## Problems I faced for the code block  ##\n",
    "It works perfectly fine like it should, saving the checkpoints, saving the conversations for the particular thread, **but** when I don't give this an input and directly hit enter, then the problem arises (you can try and run the whole level 3 and giving no input and removing the solution block).I think that the chatbot continuosly refers to the previous messages (whenever there is a new input) by accessing the memory, that means everytime it is trying to access '' (empty string) and getting an error. This is what I think is happening but I am not very sure. This theory explains the unusual repeated error messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd5113ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def stream_graph_updates(user_input: str):\\n    print(f\"User: {user_input}\")\\n    config = {\"configurable\": {\"thread_id\": \"2\"}}\\n    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]},config=config):\\n        for value in event.values():\\n            if \"messages\" in value:\\n                last_message = value[\"messages\"][-1]\\n\\n                # If it is calling a tool\\n                if isinstance(last_message, AIMessage) and last_message.tool_calls:\\n                    print(\"Assistant: Giving info to tool\")\\n\\n                # The tool has retured it\\'s result\\n                elif isinstance(last_message, ToolMessage):\\n                    tool_output = last_message.content\\n                    print(f\"Assistant:Tool response: {tool_output}\")\\n\\n                # Answer by chatbot\\n                elif isinstance(last_message, AIMessage) :\\n                    print(f\"Assistant: {last_message.content}\")\\n\\nwhile True:\\n    try:\\n        user_input = input(\"\")\\n        if user_input.lower() in [\"quit\", \"exit\", \"q\", \"chal bye\",\"alvida\",\"chal theek hai\"]:\\n            print(\"Goodbye!\")\\n            break\\n\\n        stream_graph_updates(user_input)\\n    except Exception as e:\\n        user_input = \"What is 1+2*3?\"\\n        stream_graph_updates(user_input)\\n        break'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PROBLEM BLOCK:\n",
    "\"\"\"def stream_graph_updates(user_input: str):\n",
    "    print(f\"User: {user_input}\")\n",
    "    config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]},config=config):\n",
    "        for value in event.values():\n",
    "            if \"messages\" in value:\n",
    "                last_message = value[\"messages\"][-1]\n",
    "\n",
    "                # If it is calling a tool\n",
    "                if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "                    print(\"Assistant: Giving info to tool\")\n",
    "                    \n",
    "                # The tool has retured it's result\n",
    "                elif isinstance(last_message, ToolMessage):\n",
    "                    tool_output = last_message.content\n",
    "                    print(f\"Assistant:Tool response: {tool_output}\")\n",
    "\n",
    "                # Answer by chatbot\n",
    "                elif isinstance(last_message, AIMessage) :\n",
    "                    print(f\"Assistant: {last_message.content}\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"\")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\", \"chal bye\",\"alvida\",\"chal theek hai\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        stream_graph_updates(user_input)\n",
    "    except Exception as e:\n",
    "        user_input = \"What is 1+2*3?\"\n",
    "        stream_graph_updates(user_input)\n",
    "        break\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c05e258",
   "metadata": {},
   "source": [
    "## Solution ##\n",
    "\n",
    "The problem is providing empty string. So just don't provide empty strings, if a string is empty, hard code the user input with default input. I have commented out the changes I have made. Now, this works fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbd56f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: hey there\n",
      "Assistant: Hi there! How can I help you today?\n",
      "User: I am anay\n",
      "Assistant: Nice to meet you, Anay! I'm here to assist you with any questions or tasks you might have. Just let me know what you need.\n",
      "User: do you remember my name?\n",
      "Assistant: Yes, your name is Anay.\n",
      "User: okay and what all have we talked about?\n",
      "Assistant: We've established that your name is Anay, and I confirmed that I remember it. I also offered my assistance with any questions or tasks you might have. Is there anything specific you'd like to discuss or any way I can help you further?\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "#SOLUTION BLOCK:\n",
    "def stream_graph_updates(user_input: str):\n",
    "    print(f\"User: {user_input}\")\n",
    "    config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]},config=config):\n",
    "        for value in event.values():\n",
    "            if \"messages\" in value:\n",
    "                last_message = value[\"messages\"][-1]\n",
    "\n",
    "                # If it is calling a tool\n",
    "                if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "                    print(\"Assistant: Giving info to tool\")\n",
    "                    \n",
    "                # The tool has retured it's result\n",
    "                elif isinstance(last_message, ToolMessage):\n",
    "                    tool_output = last_message.content\n",
    "                    print(f\"Assistant:Tool response: {tool_output}\")\n",
    "\n",
    "                # Answer by chatbot\n",
    "                elif isinstance(last_message, AIMessage) :\n",
    "                    print(f\"Assistant: {last_message.content}\")\n",
    "\n",
    "while True:\n",
    "   # try:\n",
    "        user_input = input(\"\")\n",
    "        if user_input.strip() == \"\":\n",
    "            print(\"Assistant: As there is no input, I will take the default input\")\n",
    "            # default input:\n",
    "            user_input = \"What is 1+2*3?\"\n",
    "            \n",
    "        elif user_input.lower() in [\"quit\", \"exit\", \"q\", \"chal bye\",\"alvida\",\"chal theek hai\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        stream_graph_updates(user_input)\n",
    "    #except Exception as e:\n",
    "       # user_input = \"What is 1+2*3?\"\n",
    "        #stream_graph_updates(user_input)\n",
    "        #break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb58a44",
   "metadata": {},
   "source": [
    "## Level 4: The Architectâ€™s Trial â€“ Multi-Agent Evolution ##\n",
    "\n",
    "This one was the most challenging one yet. Took me a whole day for this level ðŸ‘€. But made my understanding of graph builder strong. So here's my journey for this level:\n",
    "\n",
    "- Read multi-agent subsection from the document but didn't understand it properly. So referred to a couple of videos but the best one was:\n",
    "    https://www.youtube.com/watch?v=JeyDrn1dSUQ It also had another repo in it's description (https://github.com/langchain-ai/langgraph-swarm-py) I first tried building a sample one from this and it worked!\n",
    "\n",
    "- Now, the idea, I thought of building a wardrobe chatbot, but multiagents weren't needed for it, it could be done by using just a tool. So then I thought of building of travel packing assistant, included fashion agent, event agent and weather agent\n",
    "\n",
    "- I was thinking of making it a swarm but midway thought that a supervisor with a packing agent as the supervisor would make more sense. But I had built everything for swarm so did a mixture of both.\n",
    "\n",
    "- Do note that the event tool is for events within 1 month and weather tool is for weather forecast under 5 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bf5c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain.chat_models import init_chat_model\n",
    "import os\n",
    "from langchain_core.tools import tool, InjectedToolCallId\n",
    "import requests\n",
    "from langchain_core.messages import AIMessage, ToolMessage \n",
    "\n",
    "@tool\n",
    "def fashion_tool(region: str):\n",
    "  \"\"\"\n",
    "  This function uses the TavilySearch tool to fetch fashion-related content.\n",
    "  The input should be a string representing the region or topic of interest.\n",
    "  Output contains a JSON string of results which has a brief description of the content.\n",
    "  \"\"\"\n",
    "  from langchain_tavily import TavilySearch\n",
    "  os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "  tool_instance = TavilySearch(max_results=5, topic=\"general\", include_answer=True)\n",
    "  model_generated_tool_call = {\n",
    "    \"args\": {\"query\": f\"What kind of fashion is trending in {region}\"},\n",
    "    \"id\": \"1\",\n",
    "    \"name\": \"tavily\",\n",
    "    \"type\": \"tool_call\",\n",
    "  }\n",
    "  tool_msg = tool_instance.invoke(model_generated_tool_call)\n",
    "  return tool_msg.content\n",
    "\n",
    "@tool\n",
    "def event_tool(region: str):\n",
    "  \"\"\"\n",
    "  This function uses the TavilySearch tool to find major events which will happen in the given region in current month .\n",
    "  The input should be a string representing the region or topic of interest.\n",
    "  Output contains a JSON string of results which has a brief description of the content.\n",
    "  \"\"\"\n",
    "  from langchain_tavily import TavilySearch\n",
    "  os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "  tool_instance = TavilySearch(max_results=10, topic=\"general\", include_answer=True)\n",
    "  model_generated_tool_call = {\n",
    "    \"args\": {\"query\": f\"What major events will happen in {region} this month\"},\n",
    "    \"id\": \"1\",\n",
    "    \"name\": \"tavily\",\n",
    "    \"type\": \"tool_call\",\n",
    "  }\n",
    "  tool_msg = tool_instance.invoke(model_generated_tool_call)\n",
    "  return tool_msg.content\n",
    "\n",
    "@tool\n",
    "def weather_tool(city: str):\n",
    "  \"\"\"\n",
    "  Fetches the current weather for a given city using OpenWeather API.\n",
    "  The input should be a string representing the city name.\n",
    "  The function returns a dictionary with weather information.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    api_key = \"2e7f2624eab23a98351aea15d372983f\"\n",
    "    if not api_key:\n",
    "      return \"Error: OpenWeather API key is not defined in the environment variables.\"\n",
    "    BASE_URL = \"https://api.openweathermap.org/data/2.5/weather?\"\n",
    "    url = BASE_URL + \"appid=\" + api_key + \"&q=\" + city\n",
    "    response = requests.get(url).json()\n",
    "    return response\n",
    "  except Exception as e:\n",
    "    return f\"Error fetching weather data: {e}\"\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDEF3eBIfkVtIZj5hR-RDHdN2nkMj2Emxo\"\n",
    "model = init_chat_model(\"google_genai:gemini-2.0-flash\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "143eea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Annotated\n",
    "from langgraph.graph import StateGraph, START, MessagesState\n",
    "from langgraph.types import Command\n",
    "from langgraph.prebuilt import create_react_agent, InjectedState\n",
    "\n",
    "#create handoff tool is same as that of documentation\n",
    "\n",
    "def create_handoff_tool(*, agent_name: str, description: str | None = None):\n",
    "    name = f\"transfer_to_{agent_name}\"\n",
    "    description = description or f\"Transfer to {agent_name}\"\n",
    "\n",
    "    @tool(name, description=description)\n",
    "    def handoff_tool(\n",
    "        state: Annotated[MessagesState, InjectedState], \n",
    "\n",
    "\n",
    "        tool_call_id: Annotated[str, InjectedToolCallId],\n",
    "    ) -> Command:\n",
    "        tool_message = {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": f\"Successfully transferred to {agent_name}\",\n",
    "            \"name\": name,\n",
    "            \"tool_call_id\": tool_call_id,\n",
    "        }\n",
    "        return Command(  \n",
    "\n",
    "\n",
    "            goto=agent_name,  \n",
    "\n",
    "\n",
    "            update={\"messages\": state[\"messages\"] + [tool_message]},  \n",
    "\n",
    "\n",
    "            graph=Command.PARENT,  \n",
    "\n",
    "\n",
    "        )\n",
    "    return handoff_tool\n",
    "\n",
    "transfer_to_weather_info_agent = create_handoff_tool(\n",
    "    agent_name=\"weather_info_agent\",\n",
    "    description=\"Transfer to weather_info_agent to get current weather conditions and forecasts for a specific city.\",\n",
    ")\n",
    "transfer_to_fashion_agent = create_handoff_tool(\n",
    "    agent_name=\"fashion_agent\",\n",
    "    description=\"Transfer to fashion_agent to inquire about trending fashion and clothing styles in a specific region.\",\n",
    ")\n",
    "transfer_to_event_agent = create_handoff_tool(\n",
    "    agent_name=\"event_agent\",\n",
    "    description=\"Transfer to event_agent to discover major events happening in a specific region this month.\",\n",
    ")\n",
    "transfer_to_packing_agent = create_handoff_tool(\n",
    "    agent_name=\"packing_agent\",\n",
    "    description=\"Transfer back to packing_agent with the gathered information.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a49ed20",
   "metadata": {},
   "source": [
    "## Problem I faced in the below cell ##\n",
    "\n",
    "- nailing the prompt of each agent was very very hard. I had to assign every agent it's exact role or it wouldn't work. For ex- If I didn't tell packing agent to approach sequentially, it would crash or if I didn't tell other agents their specific role then they would also try to do different things and fail. So I did use LLM here as I wanted to be perfectly clear. I gave it my demands, that I want it to do so and so things, write it in a consise and structured way. So after a few back and forths, got the final prompt which worked. If I had made a full supervisor structure instead of swarm then it would have worked well with a short prompt but I am a bit short on time and this is also working pretty good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3abc205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Travel Packing Assistant! I can help you pack for your trip!!\n",
      "User: hey there\n",
      "Assistant: Hi! I'm here to help you prepare for your trip. To start, could you tell me where you're going and when? Also, let me know if you have any specific activities planned so I can give you the best recommendations.\n",
      "User: i am going to pune from 1 june to 5 june, no i have nothing planned\n",
      "Assistant: Okay, I have gathered the weather and fashion information for your trip to Pune from June 1st to June 5th.\n",
      "\n",
      "*   **Weather:** The weather in Pune is expected to be around 295.42K (22.27 degrees Celsius) with overcast clouds.\n",
      "*   **Fashion:** Trending styles in Pune include floral dresses, pastel hues, and linen shirts. Accessories are also popular.\n",
      "\n",
      "Now I can create a packing list for you:\n",
      "\n",
      "Based on this, here's a packing list recommendation:\n",
      "\n",
      "*   **Clothing:**\n",
      "    *   Light and breathable clothing: Pack clothes made from cotton, linen, or other breathable fabrics to stay comfortable in the warm weather.\n",
      "    *   Floral dresses, pastel hues clothing and linen shirts: To keep up with the trending fashion in Pune.\n",
      "    *   Consider packing clothes that can be layered.\n",
      "*   **Accessories:**\n",
      "    *   Sunglasses: To protect your eyes from the sun.\n",
      "    *   Jewelry: Based on the trending fashion, accessories are quite popular.\n",
      "*   **Other:**\n",
      "    *   Umbrella: Since it will be cloudy, there might be rain.\n",
      "    *   Comfortable shoes: Since you have nothing planned, you might end up walking a lot, so pack comfortable shoes.\n",
      "\n",
      "Enjoy your trip to Pune!\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "packing_agent = create_react_agent(\n",
    "    model,\n",
    "    [\n",
    "        transfer_to_weather_info_agent,\n",
    "        transfer_to_fashion_agent,\n",
    "        transfer_to_event_agent,\n",
    "    ],\n",
    "    prompt=\"\"\"You are packing_agent, the primary assistant for travel planning and packing advice.\n",
    "Your main goal is to help the user prepare for their trip by providing comprehensive information.\n",
    "To do this, you will coordinate with specialized agents: weather_info_agent, fashion_agent, and event_agent.\n",
    "\n",
    "Here is your methodical process:\n",
    "1.  Carefully understand the user's request. Identify ALL the pieces of information needed (e.g., weather for city X, fashion trends for region Y, local events for region Y during specific dates).\n",
    "2.  If multiple types of information are required from different specialist agents, create a plan to gather them SEQUENTIALLY.\n",
    "3.  Invoke the specialist agents ONE AT A TIME using their specific handoff tools. For example:\n",
    "    * First, if weather is needed, use the tool to transfer to 'weather_info_agent'. Wait for its response.\n",
    "    * Next, if fashion advice is needed, use the tool to transfer to 'fashion_agent'. Wait for its response.\n",
    "    * Then, if event information is needed, use the tool to transfer to 'event_agent'. Wait for its response.\n",
    "4.  After each specialist agent provides its information (which will appear as an observation/tool response), acknowledge that you have received it. The information will be in the 'content' of a 'tool' role message, often from the specialist agent itself or its handoff tool. Review the message history for this.\n",
    "5.  Once you have gathered ALL the necessary information from ALL the relevant specialist agents, and ONLY THEN, synthesize this information into a final, complete packing and travel recommendation for the user.\n",
    "6.  If the user asks a follow-up question that requires new or updated information from a specialist, repeat the relevant parts of this gathering process.\n",
    "Do not attempt to answer with partial information if more details are pending from other agents. Your goal is a comprehensive final recommendation.\n",
    "If you have gathered all information and are ready to provide the final answer to the user, do not use any tools. Just provide the answer.\n",
    "\"\"\",\n",
    "    name=\"packing_agent\",\n",
    ")\n",
    "\n",
    "weather_info_agent = create_react_agent(\n",
    "    model,\n",
    "    [\n",
    "        weather_tool,\n",
    "        transfer_to_packing_agent,\n",
    "    ],\n",
    "    prompt=\"\"\"You are weather_info_agent. Your sole responsibility is to provide accurate and current weather information for a given city.\n",
    "    Use the 'weather_tool' to fetch weather data.\n",
    "    Once you have the weather information from the weather_tool, your job is done. You MUST then use the 'transfer_to_packing_agent' tool to send this information back.\n",
    "    Do not add any conversational fluff after calling the transfer tool. The transfer tool is your final action.\n",
    "    Do not engage in packing advice or event planning.\n",
    "    The weather information will be the direct output of the 'weather_tool'. Report this information clearly when handing back.\n",
    "    \"\"\",\n",
    "    name=\"weather_info_agent\",\n",
    ")\n",
    "\n",
    "event_agent = create_react_agent(\n",
    "    model,\n",
    "    [\n",
    "        event_tool,\n",
    "        transfer_to_packing_agent,\n",
    "    ],\n",
    "    prompt=\"\"\"You are event_agent. Your task is to find and report major events happening in a specified region for the current month.\n",
    "    Utilize the 'event_tool' to search for relevant events.\n",
    "    After gathering the event details from the 'event_tool', your job is done. You MUST then use the 'transfer_to_packing_agent' tool to relay this information.\n",
    "    Do not add any conversational fluff after calling the transfer tool. The transfer tool is your final action.\n",
    "    Do not offer packing suggestions or weather forecasts.\n",
    "    The event information will be the direct output of the 'event_tool'. Report this information clearly when handing back.\n",
    "    \"\"\",\n",
    "    name=\"event_agent\",\n",
    ")\n",
    "\n",
    "fashion_agent = create_react_agent(\n",
    "    model,\n",
    "    [\n",
    "        fashion_tool,\n",
    "        transfer_to_packing_agent,\n",
    "    ],\n",
    "    prompt=\"\"\"You are fashion_agent. Your role is to identify and describe trending fashion and clothing styles in a given region.\n",
    "    Use the 'fashion_tool' to research current fashion trends.\n",
    "    Once you have the fashion information from the 'fashion_tool', your job is done. You MUST then use the 'transfer_to_packing_agent' tool to send this information back.\n",
    "    Do not add any conversational fluff after calling the transfer tool. The transfer tool is your final action.\n",
    "    Do not advise on packing or weather.\n",
    "    The fashion information will be the direct output of the 'fashion_tool'. Report this information clearly when handing back.\n",
    "    \"\"\",\n",
    "    name=\"fashion_agent\",\n",
    ")\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "graph_builder.add_node(\"packing_agent\", packing_agent)\n",
    "graph_builder.add_node(\"weather_info_agent\", weather_info_agent)\n",
    "graph_builder.add_node(\"event_agent\", event_agent)\n",
    "graph_builder.add_node(\"fashion_agent\", fashion_agent)\n",
    "\n",
    "graph_builder.add_edge(START, \"packing_agent\")\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "print(\"Welcome to the Travel Packing Assistant! I can help you pack for your trip!!\")\n",
    "\n",
    "def stream_graph_updates(user_input: str): \n",
    "    print(f\"User: {user_input}\")\n",
    "    config = {\"configurable\": {\"thread_id\": \"2\"}} \n",
    "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}, config=config):\n",
    "        for value in event.values():\n",
    "            if \"messages\" in value:\n",
    "                last_message = value[\"messages\"][-1]\n",
    "\n",
    "                if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "                    print(\"Assistant: Giving info to tool\")\n",
    "                    \n",
    "                elif isinstance(last_message, ToolMessage):\n",
    "                    tool_output = last_message.content\n",
    "                    print(f\"Assistant:Tool response: {tool_output}\")\n",
    "\n",
    "                elif isinstance(last_message, AIMessage):\n",
    "                    print(f\"Assistant: {last_message.content}\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\") \n",
    "    if user_input.strip() == \"\":\n",
    "        print(\"Assistant: As there is no input, I will take the default input\")\n",
    "        user_input = \"I want to go to Pune\" \n",
    "        \n",
    "    elif user_input.lower() in [\"quit\", \"exit\", \"q\", \"chal bye\",\"alvida\",\"chal theek hai\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "\n",
    "    stream_graph_updates(user_input) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9f19b8",
   "metadata": {},
   "source": [
    "# DONE #\n",
    "Learned a lot here and it was the most practical and fun mini project ever!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
